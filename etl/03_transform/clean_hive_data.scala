// Dependencies
import org.apache.spark.sql.{SparkSession, DataFrame, SaveMode}
import org.apache.spark.ml.feature.{StringIndexer, OneHotEncoder}
import org.apache.spark.ml.Pipeline
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.sql.expressions.UserDefinedFunction

// Spark Session
val spark = SparkSession.builder.appName("CleanHiveData").getOrCreate()
import spark.implicits._

// Read table generated by `load_hive_data.scala`
val ccData = spark.table("cook_county_data")

// Define the columns to one-hot encode
val columnsToEncode: Seq[String] = Seq("sentence_judge", "unit", "gender", "race", "court_name", "offense_category", "disposition_charged_offense_title", "bond_type_current")

// Transformations:
    // one-hot encoding over colunnsToEncode
    // retype `conversions` from StringType
    // Drop rows with `commitment_unit` not in `valuesToKeep`

// Function to apply one-hot encoding to a specified column
def encodeColumn(df: DataFrame, inputCol: String, outputCol: String): DataFrame = {
    // Replace NULLS with unknown
    val defaultValue = "unknown"
    val onehotDF = df.na.fill(defaultValue, Seq(inputCol))
    
    val indexer = new StringIndexer()
        .setInputCol(inputCol)
        .setOutputCol(s"${inputCol}_index")
        
    val encoder = new OneHotEncoder()
        .setInputCol(s"${inputCol}_index")
        .setOutputCol(outputCol)
        
    val pipeline = new Pipeline().setStages(Array(indexer, encoder))
    val model = pipeline.fit(onehotDF)
    model.transform(onehotDF).drop(s"${inputCol}_index")
}

// Apply one-hot encoding to each specified column
val encodedDF = columnsToEncode.foldLeft(ccData) { (df, col) =>
    encodeColumn(df, col, s"${col}_onehot")
}

// Function to modify column data type
def retypedColumns(
    df: DataFrame,
    conversions: Map[String, DataType]
): DataFrame = {
  conversions.foldLeft(df) {
    case (accDF, (inputCol, (targetDataType))) =>
      accDF.withColumn(
        inputCol,
        when(col(inputCol).isNotNull, col(inputCol).cast(targetDataType))//.otherwise(defaultValue)
      )
  }
}

// Function to convert commitment_terms and drop unusable rows
val convertCommitmentUDF = udf((term: Int, unit: String) => {
  unit.toLowerCase match {
    case "months" => term / 12.0
    case "days" => term / 365.0
    case "year(s)" => term.toDouble
    case "natural life" => term * 35.0
    case _ => term.toDouble  // Default to the original value if unit is not recognized
  }
})

// Function to replace values with the median
def replaceWithMedian(df: DataFrame, conversions: Map[String, DataType]): DataFrame = {
  conversions.foldLeft(df) { (accDF, entry) =>
    val (inputCol, targetDataType) = entry
    val replacementValue = accDF.stat.approxQuantile(inputCol, Array(0.5), 0.0)(0)

    accDF.withColumn(inputCol, when(col(inputCol).isNull, replacementValue).otherwise(col(inputCol).cast(targetDataType)))
  }
}

// Conversions variable
val conversions = Map(
  "age_at_incident" -> IntegerType,
  "bond_amount_current" -> FloatType,
  "commitment_term" -> IntegerType,
  "charge_count" -> IntegerType
)

// Run functions to clean data
val retypedDF = retypedColumns(encodedDF, conversions)
val modifiedDF = retypedDF.withColumn("commitment_term", convertCommitmentUDF($"commitment_term", $"commitment_unit"))
val valuesToKeep = Seq("Natural Life", "Year(s)", "Months", "Days")
val filteredDF: DataFrame = modifiedDF.filter(col("commitment_unit").isin(valuesToKeep: _*))
val nullFilteredDF: DataFrame = modifiedDF.filter(col("commitment_unit").isNotNull) // no null outcomes
val infillDF = replaceWithMedian(filteredDF, conversions)

// Show the result
infillDF.show()
val infillSize = infillDF.count()
println(s"infillDF size: $infillSize")

// Write DataFrame to Hive table
infillDF.write
  .mode(SaveMode.Overwrite)  // Choose the SaveMode: Overwrite, Append, ErrorIfExists, Ignore
  .saveAsTable("clean_cook_county_data")