// Dependencies
import org.apache.spark.sql.{SparkSession, DataFrame, SaveMode}
import org.apache.spark.ml.feature.{StringIndexer, OneHotEncoder}
import org.apache.spark.ml.Pipeline
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.sql.expressions.UserDefinedFunction

// Spark Session
val spark = SparkSession.builder.appName("CleanHiveData").getOrCreate()
import spark.implicits._

// Read table generated by `load_hive_data.scala`
val ccData = spark.table("cook_county_data")

// Define the columns to one-hot encode
val columnsToEncode: Seq[String] = Seq("sentence_judge", "unit", "gender", "race", "court_name", "offense_category", "disposition_charged_offense_title", "bond_type_current")

// Transformations:
    // one-hot encoding over colunnsToEncode
    // retype `conversions` from StringType
    // Drop rows with `commitment_unit` not in `valuesToKeep`
    // Group by case_id (this is a mistreatment of the data that would be resolved in a product app. i.e. there can be 1+ participants with 1+ charges per case_id)

// Function to apply one-hot encoding to a specified column
def encodeColumn(df: DataFrame, inputCol: String, outputCol: String): DataFrame = {
    // Replace NULLS with unknown
    val defaultValue = "unknown"
    val onehotDF = df.na.fill(defaultValue, Seq(inputCol))
    
    val indexer = new StringIndexer()
        .setInputCol(inputCol)
        .setOutputCol(s"${inputCol}_index")
        
    val encoder = new OneHotEncoder()
        .setInputCol(s"${inputCol}_index")
        .setOutputCol(outputCol)
        
    val pipeline = new Pipeline().setStages(Array(indexer, encoder))
    val model = pipeline.fit(onehotDF)
    model.transform(onehotDF).drop(s"${inputCol}_index")
}

// Apply one-hot encoding to each specified column
val encodedDF = columnsToEncode.foldLeft(ccData) { (df, col) =>
    encodeColumn(df, col, s"${col}_onehot")
}

// Function to modify column data type
def retypedColumns(
    df: DataFrame,
    conversions: Map[String, DataType]
): DataFrame = {
  conversions.foldLeft(df) {
    case (accDF, (inputCol, (targetDataType))) =>
      accDF.withColumn(
        inputCol,
        when(col(inputCol).isNotNull, col(inputCol).cast(targetDataType))//.otherwise(defaultValue)
      )
  }
}

// Function to convert commitment_terms and drop unusable rows
val convertCommitmentUDF = udf((term: Int, unit: String) => {
  unit.toLowerCase match {
    case "months" => term / 12.0
    case "days" => term / 365.0
    case "year(s)" => term.toDouble
    case "natural life" => term * 35.0
    case _ => term.toDouble  // Default to the original value if unit is not recognized
  }
})

// Function to replace values with the median
def replaceWithMedian(df: DataFrame, conversions: Map[String, DataType]): DataFrame = {
  conversions.foldLeft(df) { (accDF, entry) =>
    val (inputCol, targetDataType) = entry
    val replacementValue = accDF.stat.approxQuantile(inputCol, Array(0.5), 0.0)(0)

    accDF.withColumn(inputCol, when(col(inputCol).isNull, replacementValue).otherwise(col(inputCol).cast(targetDataType)))
  }
}

// Conversions variable
val conversions = Map(
  "age_at_incident" -> IntegerType,
  "bond_amount_current" -> FloatType,
  "commitment_term" -> IntegerType,
  "charge_count" -> IntegerType
)

// Run functions to clean data
val retypedDF = retypedColumns(encodedDF, conversions)
val modifiedDF = retypedDF.withColumn("commitment_term", convertCommitmentUDF($"commitment_term", $"commitment_unit"))
val valuesToKeep = Seq("Natural Life", "Year(s)", "Months", "Days")
val filteredDF: DataFrame = modifiedDF.filter(col("commitment_unit").isin(valuesToKeep: _*))
val nullFilteredDF: DataFrame = modifiedDF.filter(col("commitment_unit").isNotNull) // no null outcomes
val infillDF = replaceWithMedian(filteredDF, conversions)

// Group by case_id and take the first value for all cases (we are dropping ~18% of our data in order to simplify the problem space)
val groupDF = infillDF.groupBy("case_id").agg(
    first("case_participant_id").alias("case_participant_id"),
    first("charge_version_id").alias("charge_version_id"),
    first("updated_offense_category").alias("updated_offense_category"),
    first("bond_type_current").alias("bond_type_current"),
    first("bond_amount_current").alias("bond_amount_current"),
    first("bond_electronic_flag_current").alias("bond_electronic_flag_current"),
    first("chapter").alias("chapter"),
    first("act").alias("act"),
    first("section").alias("section"),
    first("class").alias("class"),
    first("aoic").alias("aoic"),
    first("event").alias("event"),
    first("event_date").alias("event_date"),
    first("law_enforcement_agency").alias("law_enforcement_agency"),
    first("judge").alias("judge"),
    first("unit").alias("unit"),
    first("incident_end_date").alias("incident_end_date"),
    first("received_date").alias("received_date"),
    first("charge_count").alias("charge_count"),
    first("charge_id").alias("charge_id"),
    first("offense_category").alias("offense_category"),
    first("primary_charge").alias("primary_charge"),
    first("disposition_charged_offense_title").alias("disposition_charged_offense_title"),
    first("age_at_incident").alias("age_at_incident"),
    first("gender").alias("gender"),
    first("race").alias("race"),
    first("incident_begin_date").alias("incident_begin_date"),
    first("arrest_date").alias("arrest_date"),
    first("arraignment_date").alias("arraignment_date"),
    first("sentence_judge").alias("sentence_judge"),
    first("court_name").alias("court_name"),
    first("court_facility").alias("court_facility"),
    first("sentence_phase").alias("sentence_phase"),
    first("sentence_date").alias("sentence_date"),
    first("sentence_type").alias("sentence_type"),
    first("current_sentence").alias("current_sentence"),
    first("commitment_type").alias("commitment_type"),
    first("commitment_term").alias("commitment_term"),
    first("commitment_unit").alias("commitment_unit"),
    first("length_of_case_in_days").alias("length_of_case_in_days"),
    first("felony_review_date").alias("felony_review_date"),
    first("felony_review_result").alias("felony_review_result"),
    first("sentence_judge_onehot").alias("sentence_judge_onehot"),
    first("unit_onehot").alias("unit_onehot"),
    first("gender_onehot").alias("gender_onehot"),
    first("race_onehot").alias("race_onehot"),
    first("court_name_onehot").alias("court_name_onehot"),
    first("offense_category_onehot").alias("offense_category_onehot"),
    first("disposition_charged_offense_title_onehot").alias("disposition_charged_offense_title_onehot"),
    first("bond_type_current_onehot").alias("bond_type_current_onehot")
  )

// 
val filterGroupDF = groupDF.filter(col("commitment_term") <= 100)

// 
val percentile = 0.75
val percentDF = filterGroupDF.groupBy("charge_count", "offense_category_onehot", "disposition_charged_offense_title_onehot").agg(expr(s"percentile_approx(commitment_term, $percentile)").alias("nth_percentile"))
percentDF.show()

// and `commonColumn` is the column common to both DataFrames
val joinedDF = filterGroupDF.join(percentDF, Seq("charge_count", "offense_category_onehot", "disposition_charged_offense_title_onehot"))

// Show the joined DataFrame
joinedDF.show()

// Show gorupDF and summary stats
joinedDF.show()
val joinedSize = joinedDF.count()
println(s"joinedDF size: $joinedSize")

// Write DataFrame to Hive table
joinedDF.write
  .mode(SaveMode.Overwrite)  // Choose the SaveMode: Overwrite, Append, ErrorIfExists, Ignore
  .saveAsTable("clean_cook_county_data")
  //